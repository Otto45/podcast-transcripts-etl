from typing import Dict, List

from models.podcast_episode import PodcastEpisode
from utils.transcript import match_speaker_label_with_name

from assemblyai import Transcript
import tiktoken
from langchain.text_splitter import RecursiveCharacterTextSplitter

def create_episode_document(podcast_episode: PodcastEpisode, transcript: Transcript) -> Dict[str, str]:
    document = {}

    document['podcast_name'] = podcast_episode.podcast_name
    document['title'] = podcast_episode.title
    document['audio_url'] = podcast_episode.audio_url
    document['description'] = podcast_episode.description
    
    if podcast_episode.original_guid is not None:
        document['original_guid'] = podcast_episode.original_guid
    
    if podcast_episode.link is not None:
        document['link'] = podcast_episode.link

    if podcast_episode.publish_date is not None:
        document['publish_date'] = podcast_episode.publish_date

    if podcast_episode.timestamps is not None:
        episode_timestamps = []

        for timestamp in podcast_episode.timestamps:
            episode_timestamp = {
                'title': timestamp.title,
                'timestamp_ms': timestamp.timestamp_ms
            }

            episode_timestamps.append(episode_timestamp)
        
        document['chapters'] = episode_timestamps

    else: # We are assuming the chapters were generated by AssemblyAI because the episode did not provide them
        episode_chapters = []

        for chapter in transcript.chapters:
            episode_chapter = {
                'title': chapter.headline,
                'timestamp_ms': chapter.start
            }

            episode_chapters.append(episode_chapter)

        document['chapters'] = episode_chapters
    
    episode_utterances = []
    speaker_names = [podcast_episode.podcast_host]

    if podcast_episode.guest_names is not None:
        speaker_names += podcast_episode.guest_names

    for utterance in transcript.utterances:
        episode_utterance = {
            'speaker': match_speaker_label_with_name(utterance.speaker, speaker_names),
            'text': utterance.text,
            'start_ms': utterance.start,
            'end_ms': utterance.end
        }

        episode_utterances.append(episode_utterance)
    
    document['transcript'] = episode_utterances
    
    return document

def prep_episode_document_for_vector_embedding(document: Dict[str, str]) -> tuple[List[str], List[str]]:
    openai_model = 'gpt-4'
    openai_model_context_window = 8192
    
    # We need to split the speaker's utterances into small enough chunks where more than one
    # can be in a context window at a time as the user chats with the model
    utterance_max_token_length = openai_model_context_window / 4

    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        model_name=openai_model,
        chunk_size=utterance_max_token_length,
        chunk_overlap=250,
        allowed_special="all"
    )

    encoding = tiktoken.encoding_for_model(openai_model)

    text_chunks: List[str] = []
    metadatas: List[str] = []

    for utterance in document['transcript']:
        utterance_metadata = {
            'speaker': utterance['speaker'],
            'start_ms': utterance['start_ms'],
            'end_ms': utterance['end_ms']
        }

        text = utterance['text']

        # Only split the text if the speaker's utterance is too many tokens for a chat context
        if (len(encoding.encode(text)) <= utterance_max_token_length):
            text_chunks.append(text)
            metadatas.append(utterance_metadata)
        else:
            utterance_chunks = text_splitter.split_text(text)
            metadatas_for_utterance_chunks = [utterance_metadata] * len(utterance_chunks)

            text_chunks += utterance_chunks
            metadatas += metadatas_for_utterance_chunks
    
    return text_chunks, metadatas
